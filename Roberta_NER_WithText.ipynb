{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "167150fc901e444ebf8bcf7143a257b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8be485f189ce4b7e950c57334b989601",
              "IPY_MODEL_fc5cb6b2c47247e8a59769b6e583549b",
              "IPY_MODEL_b397c4608f98450fbcb81ad28e427c04"
            ],
            "layout": "IPY_MODEL_477973dccac1415baff1c06e0d3dc836"
          }
        },
        "8be485f189ce4b7e950c57334b989601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62ec567181fa473b9267cbfa34e5bb8b",
            "placeholder": "​",
            "style": "IPY_MODEL_d4d71ee95b0847bf95f040c640457f2f",
            "value": "100%"
          }
        },
        "fc5cb6b2c47247e8a59769b6e583549b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c6b510aea040939d357e09b82ed8dd",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7554a5b794844f5482fab12edb702ee2",
            "value": 3
          }
        },
        "b397c4608f98450fbcb81ad28e427c04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31f1dab6c44a4c80b48b462b436a80e0",
            "placeholder": "​",
            "style": "IPY_MODEL_3f90e5333a8e45428725fdc2c0a56f70",
            "value": " 3/3 [00:00&lt;00:00, 114.10it/s]"
          }
        },
        "477973dccac1415baff1c06e0d3dc836": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62ec567181fa473b9267cbfa34e5bb8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4d71ee95b0847bf95f040c640457f2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10c6b510aea040939d357e09b82ed8dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7554a5b794844f5482fab12edb702ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "31f1dab6c44a4c80b48b462b436a80e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f90e5333a8e45428725fdc2c0a56f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [KerasNLP] Named Entity Recognition using RoBERTa\n",
        "\n",
        "**Author:** [Usha Rengaraju](https://www.linkedin.com/in/usha-rengaraju-b570b7a2/)<br>\n",
        "**Date created:** 2023/07/10<br>\n",
        "**Last modified:** 2023/07/10<br>\n",
        "**Description:** Named Entity Recognition using pretrained RoBERTa\n"
      ],
      "metadata": {
        "id": "EKRr1Vkvvcar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Named entity recognition (NER) is an NLP task that extracts information from text. NER detects and categorizes important information in text known as named entities.\n",
        "\n",
        "KerasNLP has a variety of pretrained models available. In this guide we create the whole NER pipeline using the pretrained Roberta Backbone.\n"
      ],
      "metadata": {
        "id": "pcanbuwJ7PUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & setup\n",
        "\n",
        "This tutorial requires you to have KeraNLP installed:\n",
        "\n",
        "```shell\n",
        "pip install keras-nlp\n",
        "```\n",
        "\n",
        "We begin by importing all required packages:"
      ],
      "metadata": {
        "id": "DmC_kCnI7VPq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kbJNQkGYVUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e1e296-a133-4095-bf28-55e5d8e27a52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/486.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h--2023-07-08 13:24:16--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py’\n",
            "\n",
            "conlleval.py        100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-08 13:24:17 (99.1 MB/s) - ‘conlleval.py’ saved [7502/7502]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -q datasets\n",
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiNd4e7hYVUV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from conlleval import evaluate\n",
        "import keras_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loading\n",
        "\n",
        "This guide uses the\n",
        "[Conll 2003 dataset](https://huggingface.co/datasets/conll2003)\n",
        "for demonstration purposes.\n",
        "\n",
        "To get started, we first download and unzip the dataset:"
      ],
      "metadata": {
        "id": "r8CwaHyl_8oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll_data = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "167150fc901e444ebf8bcf7143a257b3",
            "8be485f189ce4b7e950c57334b989601",
            "fc5cb6b2c47247e8a59769b6e583549b",
            "b397c4608f98450fbcb81ad28e427c04",
            "477973dccac1415baff1c06e0d3dc836",
            "62ec567181fa473b9267cbfa34e5bb8b",
            "d4d71ee95b0847bf95f040c640457f2f",
            "10c6b510aea040939d357e09b82ed8dd",
            "7554a5b794844f5482fab12edb702ee2",
            "31f1dab6c44a4c80b48b462b436a80e0",
            "3f90e5333a8e45428725fdc2c0a56f70"
          ]
        },
        "id": "7V_FhoYPe-EN",
        "outputId": "3cac9420-1ae0-4a9a-e365-ff90fd671384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "167150fc901e444ebf8bcf7143a257b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def export_to_file(export_file_path, data):\n",
        "    with open(export_file_path, \"w\") as f:\n",
        "        for record in data:\n",
        "            ner_tags = record[\"ner_tags\"]\n",
        "            tokens = record[\"tokens\"]\n",
        "            if len(tokens) > 0:\n",
        "                f.write(\n",
        "                    str(len(tokens))\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(tokens)\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(map(str, ner_tags))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "\n",
        "\n",
        "os.mkdir(\"data\")\n",
        "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
        "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
      ],
      "metadata": {
        "id": "B4WjrivLfAxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating the entities and tags mapping"
      ],
      "metadata": {
        "id": "Xo85Q67fh1b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tag_lookup_table():\n",
        "    iob_labels = [\"B\", \"I\"]\n",
        "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
        "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
        "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
        "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
        "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
        "\n",
        "\n",
        "mapping = make_tag_lookup_table()\n",
        "print(mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k668ZOSrfDFU",
        "outputId": "1d8d15ed-3cd6-49e5-b2f4-1815795466d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
        "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
        "\n",
        "counter = Counter(all_tokens_array)\n",
        "print(len(counter))\n",
        "\n",
        "num_tags = len(mapping)\n",
        "vocab_size = 20000\n",
        "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
        "\n",
        "lookup_layer = keras.layers.StringLookup(\n",
        "    vocabulary=vocabulary\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o11ECz4kfFPk",
        "outputId": "46b66db1-da41-46a4-99b3-ab356f3865ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
        "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
      ],
      "metadata": {
        "id": "62ohE_h4fIJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(train_data.take(1).as_numpy_iterator()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C714wpJfMhW",
        "outputId": "abf9c329-92b6-4dbd-b5d9-cb847255f0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Dataset\n",
        "\n",
        "For tokenizing the text we use the tensorflow text `Fastwordpiecetokenizer` and create the data generator for training the model.\n"
      ],
      "metadata": {
        "id": "gZAE1MFliivB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as tf_text\n",
        "tok = keras_nlp.models.BertTokenizer.from_preset(\"bert_base_en_uncased\", lowercase=True)\n",
        "tokenizer = tf_text.FastWordpieceTokenizer(tok.vocabulary)"
      ],
      "metadata": {
        "id": "Kltg-IYby3fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
        "    tokens = record[1 : length + 1]\n",
        "    # mask = tf.ones([length])\n",
        "    # print(tokens)\n",
        "\n",
        "    # tokens = tf.split(tokens, num_or_size_splits = tokens.shape[0], axis = 0)\n",
        "    tokens = tf.strings.reduce_join(record[1 : length + 1],separator=' ')\n",
        "    tokens = tokenizer.tokenize_with_offsets(tokens)[0]\n",
        "    tags = record[length + 1 :]\n",
        "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
        "    tags += 1\n",
        "    return (tokens, tags)\n",
        "\n",
        "def fil(ds):\n",
        "  return ds.filter(lambda x,y: tokenizer.tokenize_with_offsets(x)[0].shape==y.shape)\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "train_dataset = train_data.map(map_record_to_training_data)\n",
        "    # .map(lambda x, y,z: (lowercase_and_convert_to_ids(x), y,z))\n",
        "\n",
        "# train_dataset = train_dataset.apply(fil)\n",
        "val_dataset = val_data.map(map_record_to_training_data)\n",
        "    # .map(lambda x, y,z: (lowercase_and_convert_to_ids(x), y,z))\n",
        "\n",
        "# val_dataset = val_dataset.apply(fil)\n"
      ],
      "metadata": {
        "id": "Rp7HnnSPfOE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "cnt =0\n",
        "mnt= 0\n",
        "for x,y in train_dataset:\n",
        "  if x.shape == y.shape:\n",
        "    x_train.append(x)\n",
        "    y_oh=[]\n",
        "    for tag in y:\n",
        "      t = [0]*num_tags\n",
        "      t[tag]=1\n",
        "      y_oh.append(t)\n",
        "    y_train.append(y_oh)\n",
        "len(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw_I4pqZ-I_M",
        "outputId": "38f6d37e-abc0-4217-8c50-a4e16fc01e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5416"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = []\n",
        "y_val = []\n",
        "cnt =0\n",
        "mnt= 0\n",
        "for x,y in val_dataset:\n",
        "  if x.shape == y.shape:\n",
        "    x_val.append(x)\n",
        "    y_oh=[]\n",
        "    for tag in y:\n",
        "      t = [0]*num_tags\n",
        "      t[tag]=1\n",
        "      y_oh.append(t)\n",
        "    y_val.append(y_oh)\n",
        "len(x_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq260EwR8f6K",
        "outputId": "621b44f7-fc66-47db-8d34-d35f29155403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1205"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Building\n",
        "\n",
        "For this pipeline we use the `CustomNonPaddingTokenLoss` and then create the NER model. The backbone of the model is the pretrained `Roberta` model of KerasNLP with the base configuration. Then we use a Dense layer head for entity classification."
      ],
      "metadata": {
        "id": "GnNEySU_jD7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
        "    def __init__(self, name=\"custom_ner_loss\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "        loss = loss_fn(y_true, y_pred)\n",
        "        mask = tf.cast((y_true > 0), dtype=tf.float32)\n",
        "        loss = loss * mask\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "loss = CustomNonPaddingTokenLoss()"
      ],
      "metadata": {
        "id": "1xZqKuoTfa_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NERModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, num_tags, ff_dim=32\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.tokenizer_ = tokenizer\n",
        "        # self.proc = keras_nlp.models.RobertaPreprocessor.from_preset(\"roberta_base_en\")\n",
        "        self.transformer_block =keras_nlp.models.RobertaBackbone.from_preset(\"roberta_base_en\")\n",
        "        # self.transformer_block = keras_nlp.models.RobertaBackbone(vocab_size,4, num_heads, ff_dim,32,max_sequence_length=maxlen)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.flat=layers.Flatten()\n",
        "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "      # print(inputs)\n",
        "      # inputs = self.tokenizer_.tokenize_with_offsets(inputs)[0]\n",
        "      # print(inputs)\n",
        "      # print(inputs.shape)\n",
        "      mask = tf.ones_like(inputs)\n",
        "      # print(mask)\n",
        "      # inp = self.proc(inputs)\n",
        "      x = self.transformer_block([tf.expand_dims(inputs,axis=0),tf.expand_dims(mask,0)])\n",
        "      x = self.dropout1(x, training=training)\n",
        "      x = self.ff(x)\n",
        "      x = self.dropout2(x, training=training)\n",
        "      x = self.ff_final(x)\n",
        "      return x\n",
        "ner_model = NERModel(num_tags, ff_dim=64)\n",
        "# ner_model.compile(optimizer=\"adam\", loss=loss)"
      ],
      "metadata": {
        "id": "2o9E7RW-bv90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam(10e-5)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = loss\n",
        "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "7HPPJvGPMjmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = ner_model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, ner_model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, ner_model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = ner_model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "train_acc_list=[]\n",
        "train_loss_list=[]\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "    train_loss = []\n",
        "    train_loss_batch=[]\n",
        "    for step, (x_batch_train, y_batch_train) in tqdm(enumerate(zip(x_train,y_train))):\n",
        "        loss_value = train_step(x_batch_train, tf.expand_dims(y_batch_train,axis=0))\n",
        "        train_loss.append(float(loss_value))\n",
        "        train_loss_batch.append(float(loss_value))\n",
        "        if step % 1000 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, np.mean(train_loss_batch))\n",
        "            )\n",
        "            train_loss_batch=[]\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) ))\n",
        "    train_loss_list.append(np.mean(train_loss))\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "    train_acc_list.append(float(train_acc))\n",
        "    train_acc_metric.reset_states()\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TycS5BAkXmGq",
        "outputId": "6610cbf8-d23f-48c6-bd31-818f4a6bd047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:48, 48.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 0: 1.3125\n",
            "Seen so far: 1 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [01:23, 16.33s/it]WARNING:tensorflow:5 out of the last 5 calls to <function train_step at 0x7fd1ecebd6c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "5it [01:31, 13.49s/it]WARNING:tensorflow:6 out of the last 6 calls to <function train_step at 0x7fd1ecebd6c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1004it [08:09, 21.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1000: 1.0708\n",
            "Seen so far: 1001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2003it [09:04, 21.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2000: 1.0809\n",
            "Seen so far: 2001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3003it [10:05, 19.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3000: 1.0715\n",
            "Seen so far: 3001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4003it [11:07, 20.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4000: 0.9544\n",
            "Seen so far: 4001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5005it [12:19, 20.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5000: 1.0672\n",
            "Seen so far: 5001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5416it [12:41,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training acc over epoch: 0.8199\n",
            "Time taken: 761.06s\n",
            "\n",
            "Start of epoch 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3it [00:00, 20.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 0: 1.3068\n",
            "Seen so far: 1 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1003it [00:55, 21.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 1000: 1.0671\n",
            "Seen so far: 1001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2004it [01:50, 19.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 2000: 1.0768\n",
            "Seen so far: 2001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3003it [02:43, 17.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 3000: 1.0724\n",
            "Seen so far: 3001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4005it [03:39, 20.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 4000: 0.9605\n",
            "Seen so far: 4001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5004it [04:33, 19.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 5000: 1.0558\n",
            "Seen so far: 5001 samples\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5416it [04:55, 18.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training acc over epoch: 0.8199\n",
            "Time taken: 295.22s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "txt= \"eu rejects german call to boycott british lamb\"\n",
        "# Sample inference using the trained model\n",
        "sample_input = tokenizer.tokenize_with_offsets(txt)[0]\n",
        "\n",
        "output = ner_model.predict(sample_input)\n",
        "prediction = np.argmax(output, axis=-1)[0]\n",
        "prediction = [mapping[i] for i in prediction]\n",
        "\n",
        "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
        "print(sample_input)\n",
        "print(prediction)\n",
        "for tok, pred in zip(txt.split(), prediction):\n",
        "  print(tok, pred)"
      ],
      "metadata": {
        "id": "mvUDhYbgfjYx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b744823-e14f-4bdd-d45b-a93269ea945e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 4s 4s/step\n",
            "tf.Tensor([ 7327 19164  2446  2655  2000 17757  2329 12559], shape=(8,), dtype=int64)\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "eu O\n",
            "rejects O\n",
            "german O\n",
            "call O\n",
            "to O\n",
            "boycott O\n",
            "british O\n",
            "lamb O\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAUNQQCVGAuI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}